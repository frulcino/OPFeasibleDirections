\documentclass{article}
\usepackage{amsmath} %math fonts
\usepackage{mathtools} %align and coloneqq...
\usepackage{amssymb} %math symbols
\usepackage{amsthm} %proofs
\usepackage{adigraph} %to draw graphs
\usepackage{xcolor}
\usepackage{todonotes}
\newcommand{\ambrogio}[2][]{\todo[color=violet!40!,#1]{\textsf{VW:} #2}}
\newcommand{\gr}[2][]{\todo[color=green!20,#1]{\textsf{G:} #2}}

\newcommand{\printthis}[2][false]{%
	\ifbool{#1}{%
		#2%
	}{%
		% Drop it!!!!!
	}%
}% End of \printthis
%

%theorem enviroments:
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{problem}{Problem}


\newcommand{\nc}{\newcommand}


\nc{\st}{dx} %step symbols
\nc{\stc}{dc}
\nc{\sts}{ds}
\nc{\stP}{dP}
\nc{\stQ}{dQ}
\nc{\opfcost}{F} %cost of generation
\nc{\stS}{dS} %forse avrebbe avuto più senso dare un input ma ok...
\nc{\boB}{{\mathbf{B}}}
\nc{\boL}{{\mathbf{L}}}
\nc{\boY}{{\mathbf{Y}}}
\nc{\boI}{{\mathbf{I}}}
\nc{\boV}{{\mathbf{V}}}
\nc{\boS}{{\mathbf{S}}}
\nc{\boG}{\mathcal{G}}
\nc{\integers}{\mathbb{Z}}
\nc{\cN}{\mathcal{N}}
\nc{\cC}{\mathcal{C}}
\nc{\cI}{\mathcal{I}}
\nc{\cP}{\mathcal{P}}
\nc{\cE}{\mathcal{E}}
\nc{\cS}{\mathcal{S}}
\begin{document}
	
	
	
	\section{Jabr Model}
	
	
	
	For the OPF model construction we the network as directed graph
	$(\boB,\boL)$ where $\boB$ is the set of Buses and and $\boL \subset \boB \times \boB$ is the 
	set of branches of the network and for each adjacent buses $k,m$ both $(k,m)$ and $(m,k)$ are
	in $\boL$. So the line $l$ adjacent to $k,m$ is modeled by two edges in the arc $\{(k,m),(m,k)\}$.
	$L$ can be partitioned in $\boL_0$ and $L_1$ with $|L_0|=|L_1|$ where every line $l$, adjacent 
	to the buses $k,m$ and with a transformer at $k$, is oriented so that $(k,m) \in L_0$ and 
	$(m,k) \in \boL_1$. We also consider a set $\boG$ of generators, partitioned into 
	(possibly empty) subsets $\boG_k$ for every bus $k \in \boB$.
	We consider the following convex Jabr relaxation of the OPF problem:
	\begin{align}
		\label{Jabr equality model}
		\inf_{\substack{P^G_g, Q^G_g, c_{km}, \\ s_{km}, S_{km}, P_{km}, Q_{km}}} & \opfcost(x) \coloneqq \sum_{g \in \mathcal{G}}{F_g(P_g^G)} \\
		\text{Subject to:} & \; \forall km \in \boL \nonumber \\
		& c_{km}^2 + s_{mk}^2 \leq c_{kk}c_{mm} \quad \text{Jabr constraint} \label{Jabr constraint}\\
		& P_{km} = G_{kk}c_{kk}+G_{km}c_{km}+B_{km}s_{km} \label{c to P} \\
		& Q_{km} = -B_{kk}c_{kk}-B_{km}c_{km}+G_{km}s_{km} \label{c to Q}\\
		& S_{km} = P_{km} + jQ_{km} \label{PQ to S}\\
		& \text{Power balance constraints:}\; \forall k \in \boB\nonumber \\
		& \sum_{km \in L}S_{km}+P_k^L+iQ_k^L = \sum_{g \in \mathcal{G}(k)}{P_g^G} + i\sum_{g \in \mathcal{G}(k)}{Q_g^G} \label{constr: Power Flow Constraint J}\\
		& \text{Power flow, Voltage, and Power generation limits:} \nonumber \\
		& P_{km}^2 + Q_{km}^2 \leq U_{km} \label{Power Flow Magnitude Constraint J}\\
		& V_k^{\text{min}^2} \leq c_{kk} \leq V_k^{\text{max}^2} \label{Voltage Magnitude Constraint J} \\
		& P_g^{\text{min}} \leq P_g^G \leq P_g^{\text{max}}\label{Power generation Magnitude Constraint J} \\
		&  c_{kk} \geq 0 \; \\
		& V_k^{\text{max}} V_m^{\text{max}} \geq c_{km} \geq 0 \; \\ 
		& - V_k^{\text{max}} V_m^{\text{max}} \leq s_{km} \leq V_k^{\text{max}} V_m^{\text{max}} \; \\
		& c_{km} = c_{mk}, \;s_{km} = - s_{mk}.
	\end{align}
	
	This relaxation is in general not exact. We can recover exactness thanks to the following result:
	\begin{proposition}
		Model \eqref{Jabr equality model} with the additional \emph{loop constraint} \eqref{loop constraint} for every loop in a cycle basis of \((\boB,\boL)\) is exact, we refer to this new model as the \emph{Exact Jabr formulation}
		
		\begin{equation}
			\label{loop constraint}
			\sum_{k = 0}^{\lfloor n/2 \rfloor}\sum_{\substack{A \subset [n]\\|A|=2k}}(-1)^k\prod_{h \in A}s_{k_hk_{h+1}}\prod_{h \in A^c}c_{k_hk_{h+1}}=\prod_{k=1}^nc_{k_i,k_i}.
		\end{equation}
	\end{proposition}
	
	In \textcolor{red}{cite}, auxiliary branches were added to the network, dividing each loop in smaller loops, to decrease the degree of the polynomials defining the loop constraint. Then Mc Cormick linearization was applied.
	The problem with this approach is that one exiliary branch is added for every branch in the loop.
	This result suggests the following approaches to either find a feasible solution or move along the space of feasible solutions.
	\begin{itemize}
		\item Since the loop constraint is multilinear in it's variables, we can consider linear relaxations called \emph{Flower inequalities}, which generalize che classical Mc Cormick relaxations of products of variables. 
		\item Such relaxation is exact on tree Networks (also known as radial networks). Our objective is, given a network \(\cN = (\boB,\boL)\) which can also not be a tree, consider a radial subnetwork \(\cN' = (\boB,\boL')\), with \(\boL' \subset \boL \) and consider the Jabr model  on \(\cN'\).
		This solution is not necessarily feasible for the original problem \(\cN\), our objective is to iteratively recover a feasible solution for \(\cN\). 
		\vspace{1cm}
		Since the Jabr relaxation is exact on \(\cN'\) it follows that the constraints \ref{Jabr constraint} are respected, the constraints which are violated are the flow constraints on the leaves. We can try to recover feasibility my moving along the solution to the Jabr and Loop constraints.
		\item Given a feasible solution, find feasible directions.
	\end{itemize}
	
	\section{Linearization of loop constraints}
	
	To find feasible relaxations of the loop constraint we follow \textcolor{red}{cite}. It must be noted that a major difference in our approach is that the OPF is not a multilinear optimization problem. So first we show that the same results in \textcolor{red}{cite} can be applied to the OPF.
	
	Consider a set of multilinear constraints:
	\begin{align}
		\sum_{I \in \cI_j}c_I^j\prod_{v \in I_j} x_v \quad \forall I \leq b_j \quad \forall j \in \{1,2,\ldots, m\} \\
		x_v \in [l_v,u_v] \quad \forall v \in V
	\end{align}
	Where \(V\) denotes the variables and \(\cI_j \in \cP(V)\), for \(j = 1,\ldots,m\) are the variables of the monomials appearing in the j-th homogenous constraint.
	A straight forward linearization is to introduce a variable \(z_I\) for every subset \(I\) of variables appearing in the constraints. Thus we obtain the following equivalent problem.
	
	\begin{align}
		\sum_{I \in \cI_j}c_I^jz_I \leq b_j \quad \forall j \in \{1,2,\ldots, m\} \label{constr: linearized multilinear constraint} \\
		z_I = \prod_{i \in I} x_i \quad \forall I \in \cE \coloneqq \cup_{j=1}^m I_j \\
		x_v \in [l_v,u_v] \quad \forall v \in V
	\end{align}
	
	By affine afformation we can assume the variables \(x_v\) to be in the form \(c_v \in [0,1]\). Note that such affine transformations need to be handled with care, we will cover this in subsection \ref{subsection:Handling affine trasformations}.\
	Since contraint \eqref{constr: linearized multilinear constraint} is now linear, we are now interested in the linearization of the following set \(Pr \coloneqq  \{(x,z) \in [0,1]^V \times [0,1]^{\cE} \mid z_I = \prod_{i \in I} \forall I \in \cE\}\).
	If such constraints were the only ones, and if the cost was also multilinear, we would know that the solution would be on one of the vertices of the hypercube and the observation that follows would be trivially true. Since in the OPF the cost is not multilinear and there are other types of constraints we show that this is also a relaxation for \(Pr\).
	\begin{definition}[Standard form relaxation]\label{def: standard form relaxation}
		Let the polyhedral \(PrR\) be defined by the linear constraints  \eqref{constr: PrR1}-\eqref{constr: PrR4}.
		\gr[]{fixalignment}
		\begin{subequations}
			\begin{align}
				z_I &\leq x_v    && \forall v \in I \in \cE \label{constr: PrR1} \\
				z_I + \sum_{v \in I}(1-x_v) &\geq 1 &&  \forall I \in \cE \label{constr: PrR2}\\
				z_I &\geq 0  && \forall I \in \cE \label{constr: PrR3}\\
				x_v &\in [0,1]  && \forall v \in V \label{constr: PrR4}
			\end{align}
		\end{subequations}
	\end{definition}
	% \begin{proof}
		% \gr{This is actually much easier since by fixing all points except one we have an affine functions}
		% We need to show that every element \(x = (x_v,z_I)_{v \in V,I\in \cE} \in Pr\) satisfies constraints \eqref{constr: PrR1}-\eqref{constr: PrR4}.
		% Constraints \eqref{constr: PrR1}, \eqref{constr: PrR2}, \eqref{constr: PrR3} obviously hold. For constraint \eqref{constr: PrR2}, we see that for every choice of \(x_v \in [0,1], v \in I\), we have \(\prod_{v \in I} + \sum_{v \in I}(1-x_v) \geq 1\).
		% This is equivalent to showing that the function \( a(x_1,\ldots,x_n) \coloneqq \prod_{i = 1}^n x_i - \sum_{i=1}^nx_i + n -1 \geq 0\) on \([0,1]^n\).
		% This is trivially true for \(n = 1\). If \(n > 1 \), let \(HC \coloneqq [0,1]^n \) denote the hypercube, Let \(F_i \coloneqq \{x \in HC \mid x_i = 0\}\) and \(G_i \coloneqq \{x \in HC \mid x_i = 1\}\) denote the faces of the hypercube. We have that \[ HC = (HC \cap (F_1 + \langle e_1 - e_2\rangle)) \cup (HC \cap (G_2 + \langle e_1 - e_2\rangle)).\]
		% Thus it is sufficient to show that for every \(x \in F_1\) and \(y \in G_2\) the function \(f_x(u) \coloneqq a(x + u(e_2 - e_1))\) and the funciton \(g_y(v) \coloneqq a(y + v(e_1 - e_2))\) are non negative for all \(u \in [0,x_2]\) and \(v \in [x_1, 1]\) respectively. Expanding the expression of \(f_x\) we obtain:
		% \[
		% f_x(u) = a( u, x_2 - u, x_3, x_4, \ldots, x_n) = - u^2 \prod_{i=3}^nx_i + u \prod_{i=2}^nx_n - sum_{i=2}^nx_i + n - 1
		% \]
		% The second derivate of \(f_x(u)\) respect to \(u\) is thus \(f''_x(u) = -2 \prod_{i = 3} x_i \leq 0\) and is thus concave respect to \(v\). The value of \(f_x\) on the boundary points \(u =0,x_1\) is:
		% \begin{align}
			%   f_x(0) = a(0, x_2, \ldots, x_n) = \sum_{i=2}^nx_i + n - 1  \geq 0 \\
			%   f_x(x_2) = a(x_2, 0 , x_3, \ldots, x_n) = \prod_{i=2}^nx_i - \sum_{i=2}^nx_i + n - 2 \geq 0.
			% \end{align}
		% Where the second inequality is true by induction. Thus for every \(x \in F_1\) for every \(u \in [x_2,1]\) we have \(f_x \geq 0\).
		% Analogly we have that for every \(y \in G_2\) for every \(v \in [0,x_1]\) we have \(g_y \geq 0\). Thus for all \(x \in C\), \(a(x) \geq 0\) which concludes the proof. 
		% \end{proof}
	The corresponding Standard Form Relaxation for problem with homogeonus cost and constraints if often very weak. As done in \textcolor{red}{cite} we augment \ref{def: standard form relaxation} with \emph{Flower Inequalities}, which are additional inequalities valid for \(Pr\).
	Again the main difference with \textcolor{red}{cite: McCormick stikes back}, is that we cannot restict the hypercuber \(C\) to its vertices becase other point could also be optimal for the OPF problem. Se we check the additional flower inequalities are still valid for \(Pr\).
	
	\begin{definition}[extendend flower inequalities.]
		Let \(I \in \cE\) and let \(J_1,\ldots,J_k \in \cE \cup \cS\) be such that \(I \subset \bigcup_{i=1}^kJ_i\) and \(I \cap J_i \neq \emptyset\) holds for \(i = 1,2,\ldots,k\). The \emph{extended flower inequality} with center \(I\) and petals \(J_1,\ldots,J_k\) is defined as
		\begin{equation}\label{constr: extended flower inequality}
			z_I + \sum_{i=1}^k(1-z_{J_i}) \geq 1
		\end{equation}
		The extended flower relaxation \(FR \subset [0,1]^{\cE\cup\cS}\) are the elements \(x \in [0,1]^{\cE\cup\cS}\) for which all the extended flower inequalities hold.
	\end{definition}
	
	\begin{proposition}
		For all \(x \in Pr\) and  \(I \in \cE\), \(J_1,\ldots,J_k \in \cE \cup \cS\) such that \(I \subset \bigcup_{i=1}^kJ_i\) and \(I \cap J_i \neq \emptyset\) for \(i = 1,2,\ldots,k\). Then \emph{extended flower inequality} \eqref{constr: extended flower inequality} with center \(I\) and petals \(J_1,\ldots,J_k\) holds for \(x\). In particular \(Pr \subset FR\).
	\end{proposition}
	\begin{proof}
		For \(|I| = 1\) this is trivally true.
		For \(|I| = n > 1\), wlog \(I = \{1,\ldots,n\}\). We want to see that for any \(x \in C_{\cup_kJ_k} \coloneqq [0,1]^{\cup_{k=1}^KJ_K}\) we have \(a(x) = \prod_{i \in I}x_i + \sum_{k=1}^K (1-\prod_{j \in J_k}x_j) - 1 \geq 0\). Consider the face \(F = \{x \in [0,1]^{\cup_{k=1}^KJ_K} \mid x_1 = 0\}\). Then \(C_{\cup J_k} = C_{\cup J_k} \cap ( F + \langle e_1 \rangle)\).
		Thus we only need to show that for every \(x \in F\) the function \(f_x(x_1) \coloneqq a(x + x_1e_1)\) is positive. This is an affine function, thus it sufficient to show that it is positive at \(x_1 = 0\) and \(x_1 = 1\). For \(x_1 = 0\) this is trivially true. For \(x_1 = 1\), we have \(f_x(1) = \prod_{i \in I\setminus\{1\}}x_i + \sum_{k=1}^K (1-\prod_{j \in J_k \setminus\{1\}}x_j) - 1 \geq 0\) by induction on \(|I|\).
	\end{proof}
	By taking \(J_i = \{x_i\}\) for all \(x_i \in I\), since \(z_{J_i} = x_i\), we have:
	\begin{corollary}
		\gr[]{ok, to write better}
		The \emph{standard form relaxation} as in defition \ref{def: standard form relaxation} is a relaxation of \(Pr\). That is \(Pr \subset PrR\).
	\end{corollary}
	
	\gr[inline]{Il resto del paper McCormick strikes back drovebbe valere anche qui, perchè parla della struttura dei rilassamenti, che non dipende da Pr.}
	
	\subsection{Handling affine trasformations}\label{subsection:Handling affine trasformations}
	In the beginning of the sections, we assumed that the variables \(x_i \in [0,1]\) because affine transformation of homogenous constraints remain homogenous.But it must be noted that for each non linear affine trasformation, that is when the lower bound of the corresponding variable is not zero, the number of monomials increases. More precisly, given a monomial defined by \(I \in \cE\), let \(I' \subset I\) be the subset of variables in \(I\) for which a nonlinear trasformation is applied. Then the monomial \(I\) is split into \(2^{|I'| +1}\) new monomials. When the size of such \(I'\) is large this greatly increases the number of auxiliary variables \(z_J\) which must be introduced.
	Thus applying many non linear affine transformation can be very costly and complicates the handling of the constraints. For this reason, instead of applying non linear affine transformation, for each variables \(v \in V\) such that \(x_v \in [l_v,u_v]\) and \(l_v*u_v \neq  0 \), we split the problem in two new subproblems having \( v_x \in [l_v, 0]\) and \(v_x \in [0, u_v]\) respectively. This way linear transformations can be applied in the subproblems. This creates many subproblems, many of which are unfeasible for the OPF, we diminish the number of subproblems we need to solve thanks to some unfeasibility conditions. Then, instead of solving each subproblem in a random order, we rewrite the subproblems as a unique mixed integer programming problem.
	
	\begin{observation}
		\gr[inline]{add observation that they cannot be all positive or all negative. Can we also say something more? maybe not}
	\end{observation}
	
	Let \(C = \{k_1,\ldots, k_n\} \subset \boB \) be a cycle.
	The variables \(s_{h}\) are in the form \( s_{h} \in [-u_{s_h}, u_{s_h}]\) where \(h = (k_i,k_{i+1})\)  for all \(i = 1,\ldots,n\). We can then substitute \(s_{h}\) with \( u_h s'_h = s_h \) where
	\(s'_h \in [-1,1]\). 
	We then define the sign variables \(\sigma_h \in \{0,1\}\) for each \(h \in C\), where \(\sigma_h = 0\) if \(s_h\) is negative and \(1\) if it is positive.
	We can now rewrite the loop constraint as:
	
	% Then for each \(M \subset branches(C)\) we define the problem \(\cC(M)\) obtaining from the OPF problem by restricting the domain of 
	% \(s'_h\) variables to \(s'_h \in [-1,0]\) if \(h \in M\) and \(s'_h \in [0,1]\) if \(h \in M^c\). We observe that since the monomials appearing in each possible subproblem \(\cC(M)\) are the same, the flower inequalities are the same. The only constraint which is written differently is the loop constraint. Let \(\sigma_h^M \in \{-1,1\}\) be the variable which is equal to \(-1\) if \(h \in M\) and equal to \(0\) if \(h \in M^c\). Then the loop constraint associated to \(\cC(M)\) is 
	\begin{equation}
		\sum_{k = 0}^{\lfloor n/2 \rfloor}\sum_{\substack{A \subset [n]\\|A|=2k}}(-1)^k (\prod_{h \in A} (2\sigma_h-1) u_h) z_A=z'_C
	\end{equation}
	Where we substitute the monomial \(\prod_{h \in A}s_{k_hk_{h+1}}\prod_{h \in A^c}c_{k_hk_{h+1}}\) with \(z_A\) and the product 
	\(\prod_{k=1}^nc_{k_i,k_i}\) with \(z'_C\). For each even subset \(A \subset [n]\) we introduce the binary variable \(\lambda_A \in \{0,1\}\) which is \(0\) if \(\prod_{h \in A} (2\sigma_h-1)\) is \(-1\) and \(\lambda_A = 1\) otherwise.
	The loop constraint becomes:
	\begin{equation}
		\sum_{k = 0}^{\lfloor n/2 \rfloor}\sum_{\substack{A \subset [n]\\|A|=2k}}(-1)^k (2\lambda_A - 1)U_A z_A=z'_C
	\end{equation}
	The product \( \lambda_Az_A\) can easily be linearized. To enforce the relation  \(2\lambda_A - 1 = \prod_{h \in A}(2\delta_h -1)\), simply note that \(\lambda_A = 0\) if and only if there is an odd number of \(\delta_h\) equal to \(0\), that is, there exists \(m_A \in \integers \) such that:
	\begin{equation}
		\lambda_A + 2m_{A} = \sum_{h \in A} \delta_h 
	\end{equation}
	\gr[]{In realtà questo non è specifico all'OPF, va bene per tutti i vincoli omogenei e si può fare al posto di fare le trasformazioni affini! }
	
	% The parameters \(\lambda_A^M\) fully capture the differences between the loop constraints in the various \(\cC(M)\).
	% We consider these subproblems as a unique MILP problem the following way:
	% We introduce a binary variable \(delta_h \in \{0,1\}\) for each \(h \in branches(C)\) which correspond to the sign of \(s_h\) for all \(h \in branches(C)\).
	
	
	% Now, we can either solve all the subproblems in parallel and take the minimum or consider an associated MILP formulation by:
	% \begin{enumerate}
		%   \item Add one binary variable of each addend, that is more or less\( 2^C/2\)  + some binding constraints for the signs (which might be the same as the extended flower constraints) constraints: \gr[inline]{Question: given the loop constraint, if I randomly choose the signs of the monomials is it always one of the subproblems or is there one for which this isn't the case? Probably the latter since the number os subproblems is \(2^n\) while the number of possible signs is the number of addents in the sum which is much larger.}
		%   Now let's consider \(\lambda_A^M\) as binary variables. We want to linearize the product \(\lambda_A^M z_A\), we "can assume" \(\lambda_A^M\) to be binary. We can then linearize the product \(\lambda_A^M z_A\).
		%   \item Add one binary for each possible A (that is \(2^C\) variables) + \(2*2^C\) constraints
		% \end{enumerate}
	
	
	
	
		

	\section{Other cuts}
	Let \(I\) a set of indices and \(x_v\), \(v \in I\), continuous variables such that \(x_v \in [l_v, u_v] \subset [-1, 1]\). Let also the set \(I\) be partitioned in two set such that  \(I = J \oplus K\) and 
	\begin{subequations}
		\begin{align}
			& l_v > 0, & u_v = 1, && \forall v \in J, \label{eq:cos-var}\\
			& l_v = - u_v, & u_v < 1,&&  \forall v \in K. \label{eq:sin-var}
		\end{align}
	\end{subequations}
	If we define \(z_I \coloneqq \prod_{v \in I} x_v\) and \(I' \coloneqq I \setminus \{v\}\), then the following lower and upper bounds trivially hold:
	\begin{subequations}
		\begin{align}
			& z_I \leq x_v \prod_{v' \in I'} u_{v'}, & \forall v \in J, \\
			& z_I \leq |x_v| \prod_{v' \in I'} u_{v'}, & \forall v \in K, \\
			& z_I \geq - |x_v| \prod_{v' \in I'} u_{v'}, & \forall v \in K, \\
			& z_I \geq x_v\prod_{v' \in I'} l_{v'}, & \forall v \in J, \, I'= J' \oplus K' \, : \, K' = \emptyset, \\
			& z_I \geq - x_v \prod_{v' \in I'} u_{v'}, & \forall v \in J, \, I'= J' \oplus K' \, : \, K' \neq \emptyset.
		\end{align}
	\end{subequations}
	
	\begin{lemma}
		The following inequality holds:
		\textcolor{violet}{\textbf{Questo considera l'iperpiano tangente nel punto avente coordinate \(u_v, u_v, \ldots, u_v, \prod u_v\). Si riesce a fare lo stesso per altri punti del cuboide? Possibile che qualcuno lo ha già fatto?}}

		\begin{equation}
			z_I + \sum_{v \in I} c_v (u_v - x_v) \geq \prod_{v \in I} u_v, \quad c_v \coloneqq \prod_{v' \in I \setminus \{v\}}u_{v'}. \label{eq:ineq-lb}
		\end{equation}
		
	\end{lemma}
	\begin{proof}
		Because we are dealing with a multilinear inequality, it is sufficient to verify that it holds for every vertex of the multidimensional rectangular cuboid
		\begin{equation*}
			\mathfrak{C} \coloneqq \prod_{v \in I} [l_v, u_v] \subset [-1, 1]^{|I|}.
		\end{equation*} For such a vertex \(x\), we have either \(x_v = l_v\) or \(x_v = u_v\) for every \(v \in I\). Define \(I_1 \coloneqq \{v \in I \, | \, x_v = l_v\}\) and \(I_2 \coloneqq \{v \in I \, | \, x_v = u_v\}\), and \(J_1, J_2, K_1, K_2\) analogously. By defining \(k_1 \coloneqq |K_1| \), we then have
		\begin{align*}
			z_I + \sum_{v \in I} c_v (u_v - x_v) & = \prod_{v \in I_1} l_v \prod_{v \in I_2} u_v+ \sum_{v \in I_1} c_v (u_v - x_v) + \sum_{v \in I_2} c_v(u_v - x_v) = \\
			& =  \prod_{v \in I_1} l_v \prod_{v \in I_2} u_v+ \sum_{v \in I_1} c_v(u_v - l_v)= \\
			& =   (-1)^{k_1}\prod_{v \in K_1 \cup I_2}u_v \prod_{v \in J_1} l_v+ \sum_{v \in J_1} c_v(1 - l_v) + 2\sum_{v \in K_1}c_v u_v = \\
			& = A + B + C.
		\end{align*}
		Now, we rewrite each of the three terms. First,
		\begin{align*}
		A & = (-1)^{k_1}\prod_{v \in K_1 \cup I_2}u_v \prod_{v \in J_1} l_v = (-1)^{k_1}\prod_{v \in K_1 \cup I_2}u_v \prod_{v \in J_1}u_v \prod_{v \in J_1} l_v = \\
		& =  (-1)^{k_1}\prod_{v \in I}u_v \prod_{v \in J_1} l_v,
		\end{align*}
		where the second equality holds because \(u_v = 1, \forall v \in J_1\). Second,
		\begin{align*}
			B & = \sum_{v \in J_1} c_v(1 - l_v) = \sum_{v \in J_1} (\prod_{v' \in I \setminus \{v\}}u_{v'})(1 - l_v) =
			\sum_{v \in J_1} (\prod_{v' \in I }u_{v'})(1 - l_v) = \\
			& =(\prod_{v \in I }u_v) \sum_{v \in J_1} (1 - l_v),
		\end{align*}
		where the third equality holds because \(u_v = 1 \, \forall v \in J_1\). Finally,
		\begin{align*}
			C & = 2\sum_{v \in K_1}c_v u_v  = 2 \sum_{v \in K_1} (\prod_{v' \in I \setminus \{v\}}u_{v'})u_v =
			2 \sum_{v \in K_1} (\prod_{v' \in I }u_{v'})  = \\ 
			& = 2k_1 (\prod_{v \in I }u_v).
		\end{align*}
		We then have 
		\begin{align*}
				z_I + \sum_{v \in I} c_v (u_v - x_v) &= A+B+C = \\
				& = (\prod_{v \in I }u_v) ((-1)^{k_1} \prod_{v \in J_1} l_v + \sum_{v \in J_1} (1 - l_v) +  2k_1).
		\end{align*}
		To conclude, we just need to prove that
		\begin{equation*}
			(-1)^{k_1} \prod_{v \in J_1} l_v + \sum_{v \in J_1} (1 - l_v) +  2k_1 \geq 1,
		\end{equation*} 
		but this is true because
		\begin{align*}
			(-1)^{k_1} \prod_{v \in J_1} l_v + \sum_{v \in J_1} (1 & - l_v)  +  2k_1 = \\
			& = ((-1)^{k_1}-1) \prod_{v \in J_1} l_v + 2k_1 + \prod_{v \in J_1} l_v + \sum_{v \in J_1} (1 - l_v) =  \\
			& \geq ((-1)^{k_1}-1) \prod_{v \in J_1} l_v + 2k_1 + 1 \geq (-1)^{k_1} + 2k_1 \geq 1,
		\end{align*}
		where the first inequality holds because \(\prod_{v \in J_1} l_v + \sum_{v \in J_1} (1 - l_v) \geq 1\) because of the standard multilinear relaxation. \textcolor{violet}{cita}, the second inequality holds because \(0 < l_v < 1\) and so \(0 \leq \prod_{v \in J_1} l_v < 1\), and the last inequality holds because \(k_1 \geq 0\) integer.
%		We divide the proof in three cases, namely: \((i)\) \(K_1 = \emptyset \), \((ii)\) \(|K_1| \geq 2\) even, \((iii)\) \(|K_1| \) odd.
%		\begin{itemize}
%			\item[\((i)\)] \(K_1 = \emptyset \), for which we have
%			\begin{align*}
%				z_I + \sum_{v \in I} (u_v - x_v) & = \prod_{v \in I_2}u_v \prod_{v \in J_1} l_v+ \sum_{v \in J_1} (1 - l_v) =\\
%				&= \prod_{v \in K_2}u_v \prod_{v \in J_1} l_v+ \sum_{v \in J_1} (1 - l_v) =\\
%				& \geq \prod_{v \in K_2}u_v \prod_{v \in J_1} l_v+ \prod_{v \in K_2}u_v\sum_{v \in J_1} (1 - l_v) = \\
%				&= \prod_{v \in K_2}u_v (\prod_{v \in J_1} l_v+ \sum_{v \in J_1} (1 - l_v)) = \\
%				&\geq \prod_{v \in K_2}u_v = \prod_{v \in K}u_v = \prod_{v \in I}u_v,
%			\end{align*}
%			where the last inequality hold because of the standard relaxation of multilinear polytope. \textcolor{red}{cita}
%			\item[\((ii)\)]  \(|K_1| \geq 2\) even, for which we have
%			\begin{align*}
%				z_I + \sum_{v \in I} (u_v - x_v) & = \prod_{v \in K_1 \cup I_2}u_v \prod_{v \in J_1} l_v+ \sum_{v \in J_1} (1 - l_v) + 2\sum_{v \in K_1}u_v =\\
%				& \geq 2\sum_{v \in K_1}u_v  \geq  \prod_{v \in I} u_v,
%			\end{align*}
%			where the last inequality holds because \(0 < u_v \leq 1\).
%			\item[\((iii)\)]  \(|K_1| \) odd, let \(w \in K_1\), then we have
%			\begin{align*}
%				z_I + \sum_{v \in I} (u_v - x_v) & =  -\prod_{v \in K_1 \cup I_2}u_v \prod_{v \in J_1} l_v+ \sum_{v \in J_1} (1 - l_v) + 2\sum_{v \in K_1}u_v = \\
%				& \geq -u_w + 2 u_w = u_w \geq \prod_{v \in I} u_v
%			\end{align*}
%		\end{itemize}
%		This concludes the proof.
	\end{proof}
	\begin{observation}
		Note that, with the same notation of the proof above, \eqref{eq:ineq-lb} is tight for every vertex of \(\mathfrak{C}\) for which \(|I_1| \leq 1\).
	\end{observation}

\subsection{A new(?) class of contraints}
\newcommand{\aop}{a^{\text{op}}}
\newcommand{\prodset}{Pr}
\newcommand{\pisum}[1]{\prod_{v \in I\setminus{\{#1\}}}a_v (\aop_{#1}-a_{#1})}
We now consider when, given a vertex \(a\) of the cuboid \(\mathfrak{C}\), we can define a separating hyperplane \(\pi\) for \(\prodset\), that is, when \(\pi: \mathbb{R}^n \to \mathbb{R}\)
such that either \(\pi(x) \leq \prod_{v \in I}x_v\) for all \(x\) in \(\mathfrak{C}\) for all \(x\) in the vertex or \(\pi(x) \geq \prod_{v \in I}x_v\)
for all \(x\) in \(\mathfrak{C}\). Furthermore, we want a good cut, so we look for hyperplanes such that \(\pi(y) = \prod_{v \in I}y_v\) for all vertices \(y\) in \(\mathfrak{C}\), adjacent to \(a\). 
%
In this section we denote the vertex opposite to \(a\), \(\aop\). Choosing a vertex, corresponds to picking for each interval \(v \in I\), 
either the lower bound or the upper bound, so \(\aop\) is the vertex having as \(\aop_v = l_v\) if \(a_v = u_v\) and \(\aop_v = u_v\) otherwise.
%
Thus by defining \(d_v \coloneqq (\aop_v - a_v)e_v\), a vertex \(x \in \mathfrak{C}\) corresponds to a unique \(J \subset I\) such that \[x = a + \sum_{v \in J} d_v\]
for this reason we denote \(x\) as \(x_J\).
%
First we observe that since in \(\mathbb{R}^{n+1}\) fixed \(n+1\) affinely independent points there exists a unique hyperplane containing all these points, then there exists a unique hyperplane such that \(\pi(y) = \prod_{v \in I}y_v\) for all vertices \(y\) in \(\mathfrak{C}\), adjacent to \(a\) and for \(y = x\).
\begin{observation}
	The hyperplane \(\pi_a(x) \coloneqq \prod_{v \in I}a_v + \sum_{v \in I} C_v a_v(x_v - a_v)\), where \(C_v \coloneqq \prod_{v' \in I \setminus \{v\}}\), is the only hyperplane such that \(\pi(y) = \prod_{v \in I}y_v\) for all vertices \(y\) in \(\mathfrak{C}\), adjacent to \(a\).
\end{observation}
%
We now look for conditions for when this uniquely defined hyperplane is also a separating hyperplane for \(\prodset\).
%
\newcommand{\prodi}[1]{p(#1)}
\begin{proposition}
	\label{prop: sufficient and necessary sep hyperplane}
	The hyperplane \(\pi_a\) is a separating hyperplane for \(\prodset\) if and only if either for all \(J \subset I\), \(J = \{j_1,\ldots,j_s\}\), \(k = 1,\ldots,s-1\) and \(J_k \coloneqq \{j_1,\ldots,j_k\}\), the following holds:
	\begin{equation}
		\sum_{k = 1}^{s-1} \left(\prod_{\substack{v \in I \\ v \neq j_k}}a_v - \prod_{\substack{v \in I \\ v \neq j_k}}{x^{J_k}_v}\right) (\aop_{j_s}-a_{j_s})\geq 0
	\end{equation}
	or for all \(J \subset I\), \(J = \{j_1,\ldots,j_s\}\): 
	\begin{equation}
		\sum_{k = 1}^{s-1} \left(\prod_{\substack{v \in I \\ v \neq j_k}}a_v - \prod_{\substack{v \in I \\ v \neq j_k}}x^{J_k}_v\right) (\aop_{j_s}-a_{j_s})\leq 0
	\end{equation}
	
\end{proposition}
\begin{proof}

Let \(\prodi{x} \coloneqq \prod_{v \in I}x_v\).	Observe that for all \(J \subset I\) with \(|J| = 1\), we have \(\pi(x) = \prodi{x}\).
%
 Now fix \(J \subset I\) and denote \(J_h = \{j_1,\ldots,j_h\} \subset J\) for \(h=1,\ldots,s\).
	Then we have:
	\begin{equation}
		\label{eq: supp hyperplane}
		\pi(x_J) = \sum_{h=1}^{s-1} \pi({x_{J_{h+1}}}) - \pi({x_{J_{h}}}) + \pi({x_{J_1}}) = \sum_{h=1}^{s-1}  \left(\prod_{\substack{v \in I \\ v \neq j_h}}a_v\right)(\aop_{j_h}-a_{j_h}) + \prodi{{x_{J_1}}}
	\end{equation}
	And similarly:
	\begin{equation}
		\prodi{{x_J}}  = \sum_{h=1}^{s-1} \prodi{{x_{J_{h+1}}}} - \prodi{{x_{J_{h}}}} + \prodi{{x_{J_1}}} = \sum_{h=1}^{s-1}  \left(\prod_{\substack{v \in I \\ v \neq j_h}} a_v\right)(\aop_{j_h}-a_{j_h}) + \prodi{{x_{J_1}}}
	\end{equation}
	From which the thesis follows.
	\end{proof}
Since Inequality \eqref{eq: supp hyperplane} needs to hold for all subsets of \(I\), it is in general non-trivial to assess for which vertices \(a \in \mathfrak{C}\), the hyperplane \(\pi_a\) serves as a separating hyperplane. 
However, vertices at which the product \(\pi\) attains its maximum (or minimum) value on the cuboid always define separating hyperplanes:

\begin{proposition}
	If 
	\[
	\max_{x \in \mathfrak{C}}\prod_{v \in I}x_v = \prod_{v \in I}a_v 
	\quad\text{or}\quad 
	\min_{x \in \mathfrak{C}}\prod_{v \in I}x_v = \prod_{v \in I}a_v,
	\]
	then \(\pi_a\) is a separating hyperplane. 
	%
	In particular in the first case we have \(\pi_a(x) \leq p(x)\) for all \(x\) in \(\mathfrak{C}\) and in the second case \(\pi_a(x) \geq p(x)\).

	\end{proposition}
	
	\begin{proof}
	Assume that \(\prod_{v \in I}a_v\) attains the maximum. The argument for the minimum case is analogous.
	
	By Proposition~\ref{prop: sufficient and necessary sep hyperplane}, it suffices to verify that for all \(J \subset I\) and \(j \in J\):
	\[
	\left(\prod_{\substack{v \in I \\ v \neq j}}a_v - \prod_{\substack{v \in I \\ v \neq j}}x^{J}_v\right)(\aop_j - a_j) \leq 0.
	\]
	
	First, consider the case \(\prod_{v \in I}a_v \geq 0\):
	
	\item If \(\prod_{\substack{v \in I \\ v \neq j}}a_v \geq 0\), then \(a_j \geq 0\) and \(a_j \geq \aop_j\). Since 
	\[
	a_j\prod_{\substack{v \in I \\ v \neq j}}a_v \geq a_j\prod_{\substack{v \in I \\ v \neq j}}x^{J}_v,
	\]
	it follows that 
	\[
	\prod_{\substack{v \in I \\ v \neq j}}a_v \geq \prod_{\substack{v \in I \\ v \neq j}}x^{J}_v.
	\]
	Hence,
	\[
	\left(\prod_{\substack{v \in I \\ v \neq j}}a_v - \prod_{\substack{v \in I \\ v \neq j}}x^{J}_v\right)(\aop_j - a_j) \leq 0.
	\]
	
	 If \(\prod_{\substack{v \in I \\ v \neq j}}a_v \leq 0\), then \(a_j \leq 0\) and \(a_j \leq \aop_j\). Since
	\[
	a_j\prod_{\substack{v \in I \\ v \neq j}}a_v \geq a_j\prod_{\substack{v \in I \\ v \neq j}}x^{J}_v,
	\]
	we have
	\[
	\prod_{\substack{v \in I \\ v \neq j}}a_v \geq \prod_{\substack{v \in I \\ v \neq j}}x^{J}_v,
	\]
	and again
	\[
	\left(\prod_{\substack{v \in I \\ v \neq j}}a_v - \prod_{\substack{v \in I \\ v \neq j}}x^{J}_v\right)(\aop_j - a_j) \leq 0.
	\]

	
	Now consider the scenario where \(\prod_{v \in I}a_v < 0\). In this case all signs are reversed accordingly and the argument is the same.
	
	In all cases, the required inequality is satisfied. Hence, by Proposition~\ref{prop: sufficient and necessary sep hyperplane}, \(\pi_a\) is a separating hyperplane.
	\end{proof}

We note that computing the cuts induced by the maximum and minimum values of the product 
$\prod_{v\in V} x_v$ over the given intervals can be done in $O(n)$ time.
The procedure is as follows:
\begin{enumerate}
\item 
If all interval bounds are positive, then the maximum of the product is achieved by taking the upper bound 
for each coordinate (and the minimum is achieved by taking the lower bound for each coordinate).
\item 
Otherwise, for each variable $v$, choose as $a_v$ the endpoint of its interval $\bigl[l_v,\,u_v\bigr]$ 
that has the largest absolute value, i.e.,
\[
a_v \;\coloneqq\; \underset{x\in\{l_v,u_v\}}{\mathrm{arg\,max}}\;\bigl|x\bigr|.
\]
\item 
If the number of chosen endpoints $a_v$ that are negative is even, 
then $\bigl(a_v\bigr)_{v\in V}$ gives the \emph{unique maximum} of the product; 
otherwise, it gives the \emph{unique minimum}.
\item 
If the configuration $\bigl(a_v\bigr)_{v\in V}$ yields the maximum (respectively, minimum), 
then to obtain the minimum (resp.\ maximum), simply switch one coordinate $v^*$ of $\bigl(a_v\bigr)_{v\in V}$ 
whose ratio $\tfrac{\aop_{v^*}}{a_{v^*}}$ is minimal. 
This change flips the sign of the product, 
thus moving from a maximum‐attaining point to a minimum‐attaining point (and vice versa).
\end{enumerate}

Let $\mathfrak{C}^n \subset \mathbb{R}^n$ be an $n$-dimensional cuboid (box). 
Define an $(n-1)$-dimensional face of this cuboid by
\[
\mathfrak{C}^{n-1} \;:=\; \prod_{i=1}^{n-1}[l_i,u_i]
\]
where $l_i$ is one of the boundary values in the $i$th coordinate.

\begin{enumerate}
\item \textbf{Restriction:} Any separating (supporting) hyperplane for $\mathfrak{C}^n$ 
is induces a separating hyperplane for $\mathfrak{C}^{n-1}$.

\item \textbf{Extension:} Conversely, any hyperplane that separates 
$\mathfrak{C}^{n-1}$ (i.e.\ supports it) 
can be extended to a hyperplane defined by (or ``anchored at'') 
some vertex of $\mathfrak{C}^n$.
\end{enumerate}

\noindent
Therefore, since the only hyperplanes $\pi_a$ that could possibly remain separating hyperplanes after extension to the full cuboid $\mathfrak{C}^n$ must already 
separate the $(n-1)$-dimensional face $\mathfrak{C}^{n-1}$, we proceed by induction on $n$.
\par
Given \(\pi^{n-1}_{a}\) for vertex \(a \in \mathfrak{C}^{n_1}\), consider the extended vertex \(a'=(a,a_n) \in \mathfrak{C}^{n}\), 
then we have:
\[
\pi^{n}_{a'}(x')= \pi_a^{n-1}(x)a_n+ C^{a'}_n (x_n-a_n)
\]
Assume that \(p(x) \leq \pi_{a}^{n-1}\) for all \(x \in \mathfrak{C}^{n-1}\), and \(a_n \geq 0\) (otherwise the argument is analogous), then for all \(x' \in \mathfrak{C}^{n_1} \times \{a_n\}\):
\[
p(x') = a_n p(x) \leq a_n \pi_{a}^{n-1}(x) = \pi_{a'}^n.
\]

We now try do understand when \(\pi^{n-1}_{a'}\) also separates the remaining vertices on \(\mathfrak{C}^{n-1} \times \{b_n\}\), where \(b_n = a^{op}_n\).
%
 That is when for all \(x \in \mathfrak{C}^{n-1}\) we have:
\[
b_n\prod_{i=1}^n \leq \pi^{n}_{a'} = a_n \pi^{n-1}(x) + \prod_{i=1}^{n-1}a_i(b_n-a_n)
\]
expanding the definition of \(\pi^{n-1}\):
\[
	b_n\prod_{i=1}^nx_i \leq a_n (\sum_{i=1}^{n-1}C_i^a(x_i-a_i) + p(a)) + \prod_{i=1}^{n-1}a_i(b_n-a_n)
\]
which is equivalent to:
\begin{equation}
\label{eq: hyperplane extension}
b_n(p(x)-p(a)) \leq a_n(\sum_{i=1}^{n-1}C_i^a(x_i-a_i))
\end{equation}

We now define the sets:
\begin{align}
	C^+_a \coloneqq \{x \in \mathfrak{C}^{n-1}\mid p(x) - p(a) > 0\} \\
	C^-_a \coloneqq \{x \in \mathfrak{C}^{n-1}\mid p(x) - p(a) < 0\} \\
	C^0_a \coloneqq \{x \in \mathfrak{C}^{n-1}\mid p(x) - p(a) = 0\} 
\end{align}
Then, equation \eqref{eq: hyperplane extension}, is equivalent to the following three equations holding:
\gr[]{this is not a big help because we still need to check a lot of points rip}
\begin{align}
	{b_n} \leq a_n\inf_{x \in C_a^+}\frac{\pi^{n-1}(x) - p(a)}{p(x)-p(a)} (\geq 0)\\
	{b_n} \geq a_n\sup_{x \in C_a^-}\frac{\pi^{n-1}(x) - p(a)}{p(x)-p(a)}  \\
	0 \leq \inf_{x \in C_a^0}\pi^{n-1}(x) - p(a) (\geq 0) \label{eq: 3}
\end{align}
We notice that equation \eqref{eq: 3} is always satisfied. 
%
Unfortunately, this still means that to know if the extension of \(\pi^{n-1}_{a}\) is a separating hyperplane for \(\mathfrak{C}^{n}\) we need to check \(2^{n-1}\) points.
%
However some extensions can be easily excluded, since for an adjacent vertex \(x\) of \(a\), we have \(\pi^{n_1}(x) = p(x)\), thus if \(x \in C^+_a\) we have
\[b_n \leq a_n\inf_{x \in C^+_a}\frac{\pi^{n-1}(x) - p(a)}{p(x)-p(a)} \leq a_n*1\]
otherwise if \(x \in C^-_a\) we have 
\[b_, \geq \sup_{x \in C^-_a}\frac{\pi^{n-1}(x) - p(a)}{p(x)-p(a)} \geq a_n*1\] 
In particular, if both \(C_a^+\cap \delta(a)\) and \(C_a^-\cap \delta(a)\) (where \(\delta(a) \) are the neighbouring verticeso of \(a\)) are not empty, that is, if \(a\) si neither a local maximum, nor  a local minimum,
 then the extension of \(\pi^{n-1}_{a}\) is a separating hyperplane for \(\mathfrak{C}^{n}\) if and only if \(a_n = b_n\) (thus, most often it isn't). Thus we have:
 \begin{proposition}
	If \(l_v \neq u_v\) for all \(v\), and \(a\) is a vertex of \(\mathfrak{C}\) defining a separating hyperplane \(\pi_a\), then \(a\) is either a local minimum, or a local maximum.
 \end{proposition}














We look for sufficient and necessary conditions for when if \(\pi_{\bar{a}}\) is a separating hyperplane, if \(a \in \mathfrak{C}\) is and adjacent vertex of \(a\), then also \(\pi_a\) is a separating hyperplane.


\begin{proposition}
	\label{prop: adjacent supporting hyperplanes}
	If \(a\) is an adjacent vertex of \(\bar{a}\) and \(\pi_{\bar{a}}\) is a separating hyperplane for \(\prodset\), 
	such that \(\pi_{\bar{a}}(x) \leq p(x)\) for all \(x\) in \(\mathfrak{C}\), let \(d_j \coloneqq \bar{a}^{\text{op}}_j - \bar{a}_j \geq 0 \) for all \(j\). Then \(\pi_a\) is also a separating hyperplane for \(\prodset\) such that \(\pi_{a}(x) \leq p(x)\) if and only if 
	\begin{equation}
		\label{eq: supporting hyperplane}
		C_v^ad_v \leq C_v^{\bar{a}}d_v \text{ for all } v \text{ in } J\setminus{\bar{v}}
	\end{equation}
	Conversely the proposition also works by changing the sign of all inequalities.
\end{proposition}

\begin{proof}
	Let \(\bar{v} \in [n]\) be such that \(\bar{a} - d_{\bar{v}} = a \).%
	 First we observe that \(p(\bar{a}) = p(a) - C_{\bar{v}}^ad_{\bar{v}}\), by definition of \(C_{\bar{v}}^a\).
	%
	 Let \(x^J \coloneqq \bar{a} + \sum_{j\in J}d_j\).%
	  Then if \(\bar{v}\) is contained in \(J\), we have:
\begin{align}
	\pi_a(x^J) &= p(a) + \sum_{j \in J \setminus{\bar{v}}} C_v^a  d_v\\
	&= p(\bar{a}) + C_{\bar{v}}^ad_{\bar{v}} +  \sum_{j \in J \setminus{\bar{v}}} C_v^a  d_v \label{eq: pc} \\
 &\leq p(\bar{a}) + C_{\bar{v}}^{\bar{a}}d_{\bar{v}} +  \sum_{j \in J \setminus{\bar{v}}}  C_v^{\bar{a}}  d_v = \pi_{\bar{a}}(x^J) \leq p(x^J) 	\eqref{eq: pc}
\end{align}
Where the first inequality follows direclty from the hypothesis.
Similarly the same is done when \(\bar{v} \notin J\):

\begin{align}
	\pi_a(x^J) &= p(a) - C_{\bar{v}}^ad_{\bar{v}}  + \sum_{j \in J} C_v d_v \\
	&= p(\bar{a}) +  \sum_{j \in J } C_v^a d_v  \\
	&\leq p(\bar{a}) + C_{\bar{v}}^{\bar{a}}d_{\bar{v}}  + \sum_{j \in J }  C_v^{\bar{a}} d_v = \pi_{\bar{a}}(x^J) \leq p(x^J)
\end{align}
Thus \(\pi_{a}\) is a separating hyperplane. Now we show that if the conditions doesn't hold than \(\pi_a\) is not a separating hyperplane.
Consider \(v' \in I \setminus \bar{v}\) such that \(C_v^ad_v \leq C_v^{\bar{a}}d_v > 0\), then we have:
\begin{align}
\pi_a(a - d_{\bar{v}} + d_{v'}) &= \pi_{\bar{a}}(\bar{a}) + C_{v'}^{a}d_{v'} > p(\bar{a}) + C_{v'}^{\bar{a}}d_{v'} = 
\\& = p(\bar{a} + d_{v'})= p(a - d_{\bar{v}}+ d_{v'})
\end{align} 
\end{proof}

\begin{observation}
	If \(\bar{a}_{\bar{v}}\neq 0\), Equation \eqref{eq: supporting hyperplane} is equivalent to \(\frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}} C_v^{\bar{a}} = C_v^{a}\leq C_v^{\bar{a}} \text{ for all } v \text{ in } J\setminus{\bar{v}}.\)
\end{observation}
\begin{proof}
Since \( C_{\bar{v}}^a = \prod_{\substack{j \in I \\ j \neq \bar{v}}}a_j = \prod_{\substack{j \in I \\ j \neq \bar{v}}}\bar{a}_j = C_{\bar{v}}^{\bar{a}}\), and 
 \[ C_{h}^a = \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}}\bar{a}_{\bar{v}}\prod_{\substack{j \in I \\ j \neq \bar{v}}}a_j = \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}}\bar{a}_{\bar{v}}\prod_{\substack{j \in I \\ j \neq \bar{v}}}\bar{a}_j =  \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}}C_{h}^{\bar{a}}.\]
\end{proof}

We observe that it could still be the case that, even if the assumption does not hold, \(\pi_{\bar{a}}\) could still be a separating hyperplane for \(\prodset\), but it may separate \(\prodset\) from above instead of from below. In fact, this occurs whenever there are two adjacent vertices where one attains the maximum over the product and the other the minimum, as they both define supporting hyperplanes.
%
We note that Condition \eqref{eq: supporting hyperplane} implies that if \(1 - \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}} \leq 0\), then \(p(\bar{a} + d_{\bar{v}}) \leq p(\bar{a})\). Conversely, if \(1 - \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}} \geq 0\), then \(p(\bar{a} + d_{\bar{v}}) \geq p(\bar{a})\).
%
It is natural to examine how this condition behaves when \(\bar{a}\) attains the maximum over the product, as it defines a supporting hyperplane. In this case, it is true that \(p(\bar{a} + d_j) \leq p(\bar{a})\) for all \(j \in I \setminus \{\bar{v}\}\). However, in general, we do not have \(1 - \frac{a_{\bar{v}}}{\bar{a}_{\bar{v}}} \leq 0\). If \(\bar{a}_{\bar{v}} \geq 0\), this is equivalent to \(d_{\bar{v}} \geq 0\), which would imply \(p(\bar{a} + d_{\bar{v}}) \geq p(\bar{a})\). Thus, the adjacent point still attains a maximum.
%
On the other hand, if \(\bar{a}_{\bar{v}} \leq 0\), then Condition \eqref{eq: supporting hyperplane} implies that \(p(\bar{a} + d_{\bar{v}}) \geq p(\bar{a})\). Therefore, Proposition \ref{prop: adjacent supporting hyperplanes} indicates that the only adjacent vertices to a maximizing vertex which also support \(\prodset\) from below are themselves maximizing vertices.
%


\printthis[false]{
\begin{theorem}
	\label{thm: supportinghyperplane}
	Let \(a\) be a vertex of \(\mathfrak{C}\), let \(I\) \ambrogio{occhio che I è sempre stato il set di tutti gli indici, non vorrei ci confondessimo} be the set of variables adjacent to \(a\)  and let \(\aop\) be the vertex opposite to \(a\). Then the hyperplane \(\pi(x) \coloneqq \prod_{v \in I}a_v + \sum_{v \in I} C_v a_v(x_v - a_v)\) is a separating hyperplane for \(\prodset\) if one of the following two conditions hold.
	\begin{enumerate}
		\item \label{cond:1} For all \(v \in I\), if \(a_v \geq 0\) then \(a_v = u_v\), otherwise \(a_v = l_v\).
		\item \label{cond:2}For all \(v \in I\), if \(a_v \geq 0\) then \(a_v = l_v\), otherwise \(a_v = u_v\).
	\end{enumerate}
	If Condition \(1\) holds, then \(\pi(x) \geq \prod_{v \in I}x_v\) for all \(x\) in \(\mathfrak{C}\). If Condition \(2\) holds, then \(\pi(x) \leq \prod_{v \in I}x_v\) for all \(x\) in \(\mathfrak{C}\).
\end{theorem}
\begin{proof}{(to complete)}
	We prove the theorem for \ref{cond:1}, since the product \(\prod_{v\in I}x_v\) is multilinear, it is sufficient to check that \(\pi(x) \geq \prod_{v \in I}x_v\)
	when \(x\) is a vertex of \(\mathfrak{C}\). 
	%
	Consider the graph having as nodes the vertices of \(\mathfrak{C}\) and as arcs the edges of \(\mathfrak{C}\). We proceed by induction on the minimum distance \(d_H(x,a)\) on this graph of the vertix \(x\) from the vertix \(a\).
	Since for \(k = 0\) and \(k=1\) this is true because the hyperplane is equal to the product of the variables in \(I\) by construction. 
	%
	We prove it for when \(k=3\) since it is known that when a statement is true for \(k=1,2,3\), then is true for all \(k\in \mathbb{N}\). 
	%
	We define \(d^i \in \mathbb{R}^n\) as \(d^i \coloneqq \aop_i - a\).
	%
	Then all the vertices with distance \(d_H(x,a) = 2\) can be written as \(x = a + d_i + d_j\) for some distinct \(i,j \in I\), to make explicit the dependence of the vertix from \(d_i\) and \(d_j\) we refer to \(x\) as \(x^{ij}\), we observe that the vertices \(x^{ij}\) and \(x^{ji}\) are the same. 
	%
	Thus we have that: 
	\begin{equation}
		\pi(x^{ij}) = \pi(a + d_i + d_j) = \prod_{v \in I}a_v + \pisum{i} + \pisum{j}.
	\end{equation}
	%
	We want to prove that if \(\pi(x^{ij}) \geq \prod_{v \in I}x^{ij}\) (respectively \(\leq\)), then \(\pi(x^{hk}) \geq \prod_{v \in I}x^{hk}_v\) (\(\geq\) for all \(h,k \in I\).
	%
	Since \(x^{hk} = x^{kh}\) and by transitivity of this propriety we can restrict to the case where \(h = i\).
	%
	This is equivalent to proving that \((\pi(x^{ij}) - \prod_{v \in I}x^{ij}_v) (\pi(x^{ik}) - \prod_{v \in I}x^{ik}_v) \geq 0\).
	%
	We have that: 
	\begin{align}
		\pi(x^{ij}) - \prod_{v \in I}x^{ij}_v &= \left(a_i a_j + a_j (\aop_i - a_i) + a_i (\aop_j - a_j) - \aop_i \aop_j\right) \prod_{v \neq i,j} a_v \\
		&= \left(-a_i a_j + a_i \aop_k - a_k \aop_i - \aop_k \aop_i\right) \prod_{v \neq i,j} a_v \\
		&= \left(a_i (\aop_j - a_j) - \aop_i (\aop_j - a_j)\right) \prod_{v \neq i,j} a_v \\
		&= (\aop_i - a_i) (\aop_j - a_j) \prod_{v \neq i,j} a_v
	\end{align}
	Thus:
	\begin{align}
	\pi(x^{ij}) - \prod_{v \in I}x^{ij}_v) (\pi(x^{ik}) - \prod_{v \in I}x^{ik}_v) = \\ (\aop_i - a_i)^2 (\aop_j - a_j) (\aop_k - a_k) \prod_{v \neq i,j} a_v^2 \geq 0.
	\end{align}
	The inequality holds and only if \( (\aop_j - a_j) (\aop_k - a_k) \geq 0\) for all \(i,j,k \in I\), this is true if either Condition \ref{cond:1} or Condition \ref{cond:2} hold.
\end{proof}
}
\ambrogio{scrivo delle cose qui sotto che magari possono tornare utili}
\begin{subequations}
	\begin{align*}
		f_a (x) &= \prod_{v \in I} x_v  + \sum_{v \in I}c_v (a_v - x_v) - \prod_{v \in I} a_v, \\
		g_a (x) &= \prod_{v \in I} x_v  - \sum_{v \in I}c_v x_v
	\end{align*}
\end{subequations}
we are looking for \(a \in \mathfrak{C}\) such that  either 
\begin{subequations}
	\begin{align*}
		f_a(x) &\leq 0 & \forall x \in \mathfrak{C}, \text{ or}\\
		f_a(x) &\geq 0 & \forall x \in \mathfrak{C},
	\end{align*}
\end{subequations}
that is 
\begin{subequations}
	\begin{align*}
		g_a(x) &\leq g_a(a), & \forall x \in \mathfrak{C}, \text{ or}\\
		g_a(x) &\geq g_a(a), & \forall x \in \mathfrak{C}.
	\end{align*}
\end{subequations}
So we are looking for \(a\) such that the function \(g_a\) attains its maximum or minimum at \(a\). Note that \(g_a(a) = (1 - |I|)\prod_{v \in I} a_v\). \ambrogio{quindi secondo me, col fatto che \(I \leq 2\), se \(|a_v| = u_v\) dovrebbe essere facile far vedere che quello è sempre il massimo o il minimo, dipende da quanti segni meno ci sono}
\section{Bounds on Loop constraints violation}
We want to confront different possible relaxations in order to pick the one which minimizes the violation of the loop constraint.
%
The violation of the loop constraint comes from the fact that a solution \(x\) can violate the following equality: 
\[z_I = \prod_{v \in I}x_v\]
We are thus interested in the following quantity, 
\[\epsilon_I \coloneqq \sup_{(z_I,x) \in PrR} |z_I - \prod_{v \in I}x_v|
\]
To give a bound on the violation of the loop constraint.
%
We give a lower bound on \(\epsilon_I\):

\newcommand{\convex}{\mathcal{C}}
\begin{proposition}
	\label{prop: error convexification}
	Let \(I\) be a set of variables. Then
	\[
	  \epsilon_I \geq \sup_{(z_i,x) \in \convex{(Pr)}} |z_I - \prod_{v \in I}x_v| = U_I\left(\frac{1}{|I|}\right)^{\frac{1}{|I|-1}}\left(1 - \frac{1}{|I|}\right),
	\]
	where \(U_I \coloneqq \prod_{v \in I} \max(|u_v|,|l_v|)\).
	\gr[]{mhhh se \(l_v\) è più grande di zero l'errore è più piccolo di così.}
  \end{proposition}
  
  \begin{proof}
  We start with the case where \(l_v = 0\). It can be easily (?) shown that the point which achieves the supremum is of the type \((z_I, tu_{v_1},\ldots, tu_{v_k},\ldots)\), where \(k = |I|\),
  with \(z_I = t\prod_{v \in I}u_v\). Thus, we calculate:
  \[
  \sup_{t \in [0,1]} t\prod_{v \in I}u_v - \prod_{v \in I}tu_v = \sup_{t \in [0,1]} \prod_{v \in I}u_v(t - t^k).
  \]
  The supremum is attained at \(t = \left(\frac{1}{|I|}\right)^{\frac{1}{|I|-1}}\), and thus the error is \[\prod_{v \in I}u_v\left(\frac{1}{|I|}\right)^{\frac{1}{|I|}}\left(1 - \frac{1}{|I|}\right)\].
  
  In general, for \(l_v\) possibly negative, we observe that the relaxation introduced in Section \textcolor{green}{ref} is not convex. However, if restricted to each quadrant, it is convex, and we can apply the same argument. The error is then larger than the maximum error of the convexification of the graph of the monomial over each quadrant, and thus the thesis follows.
  \end{proof}
  
  We now consider how the error on the monomial approximation influences the error on the loop constraint:
\begin{align}
	\epsilon_C &\coloneqq | \prod_{v \in V}c_{vv}- \sum_{k=0}^{\lfloor\frac{|C|}{2}\rfloor}\sum_{\substack{A \subset \cE(C) \\ |A| = 2k}}(-1)^k\prod_{e \in A}c_{e}\prod_{e \in A^c}s_e|\\
	& \leq  |z^v_C + \epsilon^v_{C}+ \sum_{k=0}^{\lfloor\frac{|C|}{2}\rfloor}\sum_{\substack{A \subset \cE(C) \\ |A| = 2k}}z_A + \epsilon_A|\\
	& \leq  |\prod_{v \in V}\epsilon^v_{C}|+ \sum_{k=0}^{\lfloor\frac{|C|}{2}\rfloor}\sum_{\substack{A \subset \cE(C) \\ |A| = 2k}}|\epsilon_A| \\
	& \leq \overline{\epsilon}_C + \sum_{k=0}^{\lfloor\frac{|C|}{2}\rfloor}\sum_{\substack{A \subset \cE(C) \\ |A| = 2k}}\overline{\epsilon}_C \cong 2^{|C|-1}\overline{\epsilon}_C
\end{align}

As done in \textcolor{green}{cite}, cycles can be decomposed into cycles of length 3 or 4 (resulting in McCormick relaxations of binomial), 
or larger cycles (resulting in generalized monomial relxations).
%
We determine the optimal length subcycle length to minimize the loop constraint violation.
%
To do this, given the constraints induced by the subcycles of \(C\) we relate these to the loop constraint over \(C\) to confront the error.
%
First, given \(C\), decomposed in the cycles \(C_0,\ldots,C_k\), let \(\epsilon_{C_i}\) be the loop constraint violation over the subcycle \(C_i\), and let \(\epsilon^{s}_{C_i}\) be the constraint violation respect to the constraint \(\prod_{v \in C_i} \sin(\sum_{e \in \cE(C_i)}\theta_e) = 0\).
Lastly, we denote by \(LHS^{c}_{C_i}\) (resp. \(RHS^{s}_{C_i}\)) the left hand side (right hand side) of the constraint \(\prod_{v \in C_i}c_{vv}\cos(\sum_{e \in \cE(C_i)}\theta_e) = \prod_{v \in C_i}c_{vv}\).
Consider:
\[
\prod_{v \in C}c_{vv}\cos(\sum_{e \in \cE(C)}\theta_e) = \prod_{v \in C}c_{vv}
\]
By expanding the cosine, and substituting \(\cos(\theta_e)|V_{e_0}||V_{e_1}|\) and  \(\sin(\theta_e)|V_{e_0}||V_{e_1}|\), by \(c_{e}\) and \(s_e\) respectively, we obtain the loop constraint.
Alternatively, observe that:
\[
\cos(\sum_{e \in \cE(C)} \theta_e) = \cos(\sum_{i = 0}^k \sum_{e \in C_i}\theta_e) = \cos(\sum_{i = 0}^k \theta_{C_i})
\]
Where \(\theta_{C_i} \coloneqq \sum_{e \in C_i}\theta_e\). 
%
Thus, by expanding the last sum respect to the angles \(\theta_{C_i}\) 
and multiplying by \(\prod_{i=0}^k\prod_{v \in C_i}c_{vv}\), 
we obtain the loop constraint expressed respect in function of the LHS of the loop constraints of the subcycles:
\[
\sum_{h=0}^{\lfloor\frac{k}{2}\rfloor}(-1)^h\sum_{\substack{A \subset [k] \\ |A| = 2h}}\prod_{i \in A}LHS^{c}_{C_i}\prod_{i \in A^c}LHS^{s}_{C_i} = \prod_{i=0}^k\prod{v\in C_i}c_{vv}
\]
Observe that \(LHS^c_{C_i} = RHS^C_{C_i} + \epsilon^s_{C_i}\) and \(LHS^s_{C_i} = \epsilon^s_{C_i}\), and that dividing by \(\prod_{i = 0}^k c_{v_i,v_i}\) we obtain the expression of the loop constraint over \(C\).
Thus:
\[
\frac{1}{\prod_{i = 0}^k c_{v_i,v_i}}\sum_{h=0}^{\lfloor\frac{k}{2}\rfloor}(-1)^h\sum_{\substack{A \subset [k] \\ |A| = 2h}}\prod_{i \in A}(RHS^{c}_{C_i}+ \epsilon_{C_i})\prod_{i \in A^c}\epsilon^s_{C_i}) = RHS_{C}
\]
This sum can be divided in two sums, one corresponding to the left hand side of the loop constraint over \(C\),  and the other corresponding to the loop constraint violation, depending on \(\epsilon^c\) and \(\epsilon^s\).
%
By taking the absolute values, we obtain:
\[
\epsilon_C \leq 2^{|k|-1} \overline{\epsilon}_{C}^A
\]
Where \(\overline{\epsilon}_{C}^A\) is the average of the products in the sum. Then, since \(k = |C|/c\), where \(c = |C_i|\) the optimal length of the subcycles is \textcolor{red}{\(c = {42*y}\) with \(y\) to be determined}.
\textcolor{red}{MHHHHH va scritto meglio}
\printthis[false]{
\section{Feasible directions}
	
	Let \(x_0 = (P_0,Q_0,c_0,s_0)\) be a feasible solution of the OPF problem \eqref{Jabr equality model} with the loop constraints \eqref{loop constraint}.
	We want to find feasible directions \(x_0 = (\stP,\stQ,\stc,\sts)\), that is such that \(x_1 = (P_0+\stP,Q_0+\stQ,c_0+\stc)\) is still a feasible solution of the OPF problem.
	We consider each constraint of the Exact Jabr Formulation separately do get feasible directions.
	
	\subsection{Jabr Constraint}
	
	Since \(x_0\), the jabr equality holds: \(c_{ii}c_{jj}= c_{ij}^2+s_{ij}^2\).
	Adding the movement \(\st\) we want that \((c_{ii}+\stc_{ii})(c_jj+\stc_{jj}) = (c_{ij}+\stc_{ij})^2+(s_{ij}+\sts_{ij})^2\). By expanding the terms and considering that the equality holds for \(x_0\) this is equivalent to:
	\begin{equation}
		\stc_{ii}\stc_{jj}+2c_{ii}\stc_{jj}+2\stc_{ii}c_{jj}= \stc_{ij}^2+c_{ij}\stc_{ij}+\sts_{ij}^2+s_{ij}\sts_{ij}
	\end{equation}
	For now we consider movements where \(\stc_{ii}\) is not zero only on an independent set of nodes in the graph, this way the constraint simplifies to: 
	\begin{equation}
		\stc_{ij}^2+c_{ij}\stc_{ij}+\sts_{ij}^2+s_{ij}\sts_{ij}-2\stc_{ii}c_{jj} = 0 
	\end{equation}
	The solutions to this constraint can be found by minimizing the following minimization problem:
	\begin{equation}
		\min  (\stc_{ij}^2+c_{ij}\stc_{ij}+\sts_{ij}^2+s_{ij}\sts_{ij}-2\stc_{ii}c_{jj})^2
	\end{equation}
	Which can be solved by gradient descent methods. Since we want to solve this for all \((i,j)\in \boL\), we instead solve the following:
	\begin{equation}
		\min  CJ(\stc,ds,\stP) = \sum_{(i,j)\in \boL} (\stc_{ij}^2+c_{ij}\stc_{ij}+\sts_{ij}^2+s_{ij}\sts_{ij}-2\stc_{ii}c_{jj})^2
	\end{equation}
	\gr[inline]{Anche questo può essere risolto con metodo gradiente? Sappiamo che il minimo globale è zero, ma possono esserci minimi locali? Dobbiamo per forze imporre gli spostamenti solo su nodi indipendenti?}
	
	
	\subsection{Loop Constraints}
	Unfortunately, the number of monomials to computer for checking the various of loop constraints grows factorially with the length of the cycle. Thus trying to apply a similar method to find feasible directions satisfying the loop constraints as we did for the Jabr constraints would be computationally intractable.
	We can try the following approaches:
	\begin{itemize}
		\item \textcolor{gray}{Find the cycle basis with minimal weight}
		\item Each cycle can be broken down by adding auxiliary edges in the Network, decomposing each loop constraints in loop contraints associated to cycle of the desired length. For each auxiliary edge \(e = (e_0,e_1)\) we add the auxiliary variable \(c_e,s_e\), the associated Jabr constraints: \(c_e^2 + s_e^2 = c_{e_0}c_{e_1}\). We can now divide the cycles containing \(e_0\) and \(e_1\) in two cycles containing the edge \(e\) and thus obtaining smaller associated loop contraints.
		\item Loop constraints of cycles of length equal to 3 or 4 can be rewritten as 2 polynomial constraints of degree 2.
		
	\end{itemize}
	
	Let us consider the loop constraints on the following cycle:
	\NewAdigraph{threecycle}{
		1:2;
		2:2;
		3:2;
	}{
		1,2;
		2,3;
		3,1;
	}[-]
	\begin{center}
		
		\threecycle{}
	\end{center}
	This corresponds to the following polynomial constraints:
	\begin{equation}
		p_3 \coloneqq c_{12}(c_{23}c_{31} - s_{23}s_{31})-s_{12}(s_{23}c_{31}+c_{23}s_{31})-c_{11}c_{22}c_{33}
	\end{equation}
	We also define the following polynomials associated to the Jabr constraints at each edge:
	\begin{equation}
		p_{ij} \coloneqq c_{ij}^2+s_{ij}^2-c_{ii}c_{jj}
	\end{equation}
	and
	\begin{align}
		q_3^1 &= s_{12}c_{33} + c_{23}s_{31} + s_{23}c_{31} \label{constr: loop decomposition 1}\\
		d_3^2 &= c_{12}c_{33} + c_{23}c_{31} + s_{23}s_{31} \label{constr: loop decomposition 2}
	\end{align}
	Then, we have the following results which let's us to rewrite the constraints associated to a cycle of length three as two polynomial constraints of degree 2.
	\begin{proposition}
		\begin{equation}
			\{(c,s)\mid p_3 = p_{12} = p_{23} = p_{31} = 0\} = \{(c,s)\mid q_3^1 = q_3^2 = p_{12} = p_{23} = p_{31} = 0\} 
		\end{equation}
	\end{proposition}
	
	If we don't restrict the possible direction \(\st=(d_c,d_s,d_P)\), imposing that constraints \eqref{constr: loop decomposition 1}, \eqref{constr: loop decomposition 2} hold for \(x + \st\) would impose polynomial constraints on \(\st\).
	Alternatively, since  for example \(q_3^1\) is a bilinear polynomial respect to the vectors \(v_1=(s_{12}, c_{23}, s_{23})\) and \(v_2=(c_{33}, s_{31}, c_{31})\), we can fix the variables in \(v_2\), that is \(dv_2 = 0\) and move along \(v_1\).
	This way constraint \(q_3^1=0\) imposes a linear constraint on \(d_x\). 
	We can proceed on a similar way for \(q_3^2\). We note that one can consider instead to move along \(v_2\) and that there are other possible choices for \(v_1\) and \(v_2\). 
	\gr[inline]{The hope is that even though we are restricting the directions for a single step, that by concatenating steps we can obtain a "good" amount of feasible directions.}
	\gr[inline]{TODO: think well about possible directions}
	
	\subsection{Flow constraints}
	The flow constraint \eqref{constr: Power Flow Constraint J} also induce linear constraints on the step \(\st\):
	\begin{align}
		\stP_{km} &= G_{kk}\stc_{kk}+G_{km}\stc_{km}+B_{km}\sts_{km}\\
		\stQ_{km} &= -B_{kk}\stc_{kk}-B_{km}\stc_{km}+G_{km}\sts_{km} \\
		\stS_{km} &= \stP_{km} + j\stQ_{km} \\
		\sum_{km \in L}\stS_{km}+\stP_k^L+i\stQ_k^L &= \sum_{g \in \mathcal{G}(k)}{\stP_g^G} + i\sum_{g \in \mathcal{G}(k)}{\stQ_g^G} \label{Power Flow Constraint J} 
	\end{align}

  	
	\subsection{Other constraints}
	Magnitude constraints are what limit the step size:
	\begin{align}
		V_k^{\text{min}^2} \leq c_{kk} + \stc_{kk} \leq V_k^{\text{max}^2}  \\
		P_g^{\text{min}} \leq P_g^G + \stP_g^G\leq P_g^{\text{max}}\\\
		c_{kk} + \stc_{kk} \geq 0 \;
	\end{align}
	\gr[inline]{what about this one?}
	\begin{equation*}
		P_{km}^2 + Q_{km}^2 \leq U_{km}
	\end{equation*}
	
	\subsection{Step Problem}
	Combining the considerations in the previous subsections we obtaing the following problem to calculate a feasible step.
	Where given a graph, we find a cycle basis, we decompose each cycle in a 3/4-cycles, for each of these smaller cycles we consider their loop constraints and pick \gr[]{how?} which variables do move in the step \(\st\).
	\gr[inline]{Da riscrivere meglio, i vincoli su \(q_e^1,q_e^2\) sono per ciascun arco in un ciclo in una base di cicli, ma se lo stesso arco è in due basi di cicli diverse, se i due cicli non hanno nodi in comune allora sono due vincoli diversi}
	
	\begin{align}
		\label{prob: Step Problem}
		\min  CJ(\st) = & sum_{(i,j)\in \boL} (\stc_{ij}^2+c_{ij}\stc_{ij}+\sts_{ij}^2+s_{ij}\sts_{ij}-2\stc_{ii}c_{jj})^2 \\
		\text{Subject to:} \nonumber & \\
		q_e^1 &= \sts_{12}c_{33} + \stc_{23}s_{31} + \sts_{23}c_{31}\\
		q_e^2 &= \stc_{12}c_{33} + \stc_{23}c_{31} + \sts_{23}s_{31} \\
		\stP_{km} &= G_{kk}\stc_{kk}+G_{km}\stc_{km}+B_{km}\sts_{km}\\
		\stQ_{km} &= -B_{kk}\stc_{kk}-B_{km}\stc_{km}+G_{km}\sts_{km} \\
		\stS_{km} &= \stP_{km} + j\stQ_{km} \\
		\sum_{km \in L}\stS_{km} &= \sum_{g \in \mathcal{G}(k)}{\stP_g^G} + i\sum_{g \in \mathcal{G}(k)}{\stQ_g^G} \\
		&V_k^{\text{min}^2} \leq c_{kk} + \stc_{kk} \leq V_k^{\text{max}^2}  \\
		&P_g^{\text{min}} \leq P_g^G + \stP_g^G\leq P_g^{\text{max}}\\\
		&c_{kk} + \stc_{kk} \geq 0 \;
	\end{align}
	
	If the optimal solutions of problem \eqref{prob: Step Problem} are feasible steps if and only if their cost is 0. 
	Leaven like this one obvious solution is \(dx = 0\). Possible ideas.
	\begin{itemize}
		\item Impose that the generation cost must decrease, by putting the scalar product of \(\st\) with the gradient of the cost function \(\opfcost\) in \(x\) to be smaller than 0. \(\st \nabla F(x) < 0 \). With this additional constraint if it's an equality then the family of the possible direction doesn't have a decreasing cost direction.
	\end{itemize}
	Since the constraints are affine, problem \eqref{prob: Step Problem} can be solved with a projected gradient descent algorithm.
	\gr[]{If we don't consider magnitude constraints, we can consider directly the projection of the gradient on the subspace of feasible solutions, this would be easier than considering the projection on a generic poligon (I think)}
	
	\newpage
}
\end{document}